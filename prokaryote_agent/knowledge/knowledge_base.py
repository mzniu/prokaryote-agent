"""
Markdown çŸ¥è¯†åº“ - ä½¿ç”¨ Markdown æ–‡ä»¶å­˜å‚¨å’Œæ£€ç´¢çŸ¥è¯†

V0.1 ç‰¹æ€§:
- Markdown æ–‡ä»¶å­˜å‚¨ï¼Œäººç±»å¯è¯»
- YAML frontmatter å…ƒæ•°æ®
- ç®€å•å…³é”®è¯æœç´¢
- è‡ªåŠ¨ç´¢å¼•ç®¡ç†
- LLM å‹å¥½æ ¼å¼
"""

import os
import re
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field, asdict


@dataclass
class Knowledge:
    """çŸ¥è¯†æ¡ç›®"""
    id: str                          # å”¯ä¸€æ ‡è¯†
    title: str                       # æ ‡é¢˜
    content: str                     # å†…å®¹
    domain: str                      # é¢†åŸŸ (legal, software_dev, ...)
    category: str                    # ç±»åˆ« (laws, cases, errors, ...)
    
    # å…ƒæ•°æ®
    keywords: List[str] = field(default_factory=list)  # å…³é”®è¯
    source_url: str = ""             # æ¥æº URL
    source_type: str = "web_search"  # æ¥æºç±»å‹
    acquired_by: str = ""            # è·å–è¯¥çŸ¥è¯†çš„æŠ€èƒ½
    quality_score: float = 0.5       # è´¨é‡è¯„åˆ† 0-1
    
    # æ—¶é—´æˆ³
    created_at: str = ""
    updated_at: str = ""
    
    # ä½¿ç”¨ç»Ÿè®¡
    access_count: int = 0
    last_accessed: str = ""
    
    def __post_init__(self):
        if not self.created_at:
            self.created_at = datetime.now().isoformat()
        if not self.updated_at:
            self.updated_at = self.created_at


class MarkdownKnowledge:
    """
    Markdown çŸ¥è¯†åº“ç®¡ç†å™¨
    
    ç›®å½•ç»“æ„:
    knowledge/
    â”œâ”€â”€ _index.md          # çŸ¥è¯†åº“ç´¢å¼•
    â”œâ”€â”€ legal/             # æ³•å¾‹é¢†åŸŸ
    â”‚   â”œâ”€â”€ laws/          # æ³•è§„
    â”‚   â”œâ”€â”€ cases/         # åˆ¤ä¾‹
    â”‚   â””â”€â”€ concepts/      # æ¦‚å¿µ
    â””â”€â”€ software_dev/      # è½¯ä»¶å¼€å‘é¢†åŸŸ
        â”œâ”€â”€ errors/        # é”™è¯¯è§£å†³
        â””â”€â”€ apis/          # API æ–‡æ¡£
    """
    
    def __init__(self, base_path: str = "prokaryote_agent/knowledge"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
        # å†…å­˜ç¼“å­˜ï¼ˆåŠ é€Ÿæœç´¢ï¼‰
        self._cache: Dict[str, Knowledge] = {}
        self._cache_loaded = False
    
    def store(self, knowledge: Knowledge) -> str:
        """
        å­˜å‚¨çŸ¥è¯†åˆ° Markdown æ–‡ä»¶
        
        Args:
            knowledge: çŸ¥è¯†å¯¹è±¡
            
        Returns:
            å­˜å‚¨çš„æ–‡ä»¶è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå®‰å…¨å¤„ç†ï¼‰
        safe_title = self._safe_filename(knowledge.title)
        file_path = self.base_path / knowledge.domain / knowledge.category / f"{safe_title}.md"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
        if file_path.exists():
            existing = self._read_knowledge_file(file_path)
            if existing and self._is_similar(existing, knowledge):
                self.logger.info(f"çŸ¥è¯†å·²å­˜åœ¨ï¼Œè·³è¿‡: {knowledge.title}")
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                self._update_access(file_path)
                return str(file_path)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆ Markdown å†…å®¹
        md_content = self._generate_markdown(knowledge)
        
        # å†™å…¥æ–‡ä»¶
        file_path.write_text(md_content, encoding='utf-8')
        self.logger.info(f"çŸ¥è¯†å·²å­˜å‚¨: {file_path}")
        
        # æ›´æ–°ç¼“å­˜
        self._cache[knowledge.id] = knowledge
        
        # æ›´æ–°ç´¢å¼•
        self._update_index()
        
        return str(file_path)
    
    def store_from_search(self, title: str, content: str, domain: str, 
                          category: str, source_url: str = "",
                          acquired_by: str = "", keywords: List[str] = None) -> str:
        """
        ä»æœç´¢ç»“æœå­˜å‚¨çŸ¥è¯†ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            title: æ ‡é¢˜
            content: å†…å®¹
            domain: é¢†åŸŸ
            category: ç±»åˆ«
            source_url: æ¥æºURL
            acquired_by: è·å–æŠ€èƒ½ID
            keywords: å…³é”®è¯åˆ—è¡¨
        """
        # ç”Ÿæˆå”¯ä¸€ID
        content_hash = hashlib.md5(f"{title}{content}".encode()).hexdigest()[:8]
        knowledge_id = f"kb_{domain}_{content_hash}"
        
        # è‡ªåŠ¨æå–å…³é”®è¯
        if keywords is None:
            keywords = self._extract_keywords(title + " " + content)
        
        knowledge = Knowledge(
            id=knowledge_id,
            title=title,
            content=content,
            domain=domain,
            category=category,
            keywords=keywords,
            source_url=source_url,
            acquired_by=acquired_by,
            source_type="web_search"
        )
        
        return self.store(knowledge)
    
    def search(self, query: str, domain: str = None, 
               category: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢çŸ¥è¯†åº“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            domain: é™å®šé¢†åŸŸï¼ˆå¯é€‰ï¼‰
            category: é™å®šç±»åˆ«ï¼ˆå¯é€‰ï¼‰
            limit: æœ€å¤§è¿”å›æ•°é‡
            
        Returns:
            åŒ¹é…çš„çŸ¥è¯†åˆ—è¡¨
        """
        results = []
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        # ç¡®å®šæœç´¢è·¯å¾„
        if domain:
            if category:
                search_path = self.base_path / domain / category
            else:
                search_path = self.base_path / domain
        else:
            search_path = self.base_path
        
        if not search_path.exists():
            return []
        
        # éå†æ‰€æœ‰ Markdown æ–‡ä»¶
        for md_file in search_path.rglob("*.md"):
            # è·³è¿‡ç´¢å¼•æ–‡ä»¶
            if md_file.name.startswith("_"):
                continue
            
            try:
                content = md_file.read_text(encoding='utf-8')
                content_lower = content.lower()
                
                # è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
                score = 0
                
                # æ ‡é¢˜åŒ¹é…ï¼ˆæƒé‡é«˜ï¼‰
                title = self._extract_title(content)
                if query_lower in title.lower():
                    score += 10
                
                # å…³é”®è¯åŒ¹é…
                keywords = self._extract_frontmatter_field(content, 'keywords')
                if keywords:
                    for kw in keywords:
                        if kw.lower() in query_lower or query_lower in kw.lower():
                            score += 5
                
                # å†…å®¹åŒ¹é…
                for word in query_words:
                    if word in content_lower:
                        score += 1
                
                # å®Œæ•´æŸ¥è¯¢åŒ¹é…
                if query_lower in content_lower:
                    score += 3
                
                if score > 0:
                    results.append({
                        'path': str(md_file),
                        'title': title,
                        'domain': self._get_domain_from_path(md_file),
                        'category': self._get_category_from_path(md_file),
                        'score': score,
                        'snippet': self._extract_snippet(content, query)
                    })
                    
            except Exception as e:
                self.logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {md_file}: {e}")
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        
        return results[:limit]
    
    def get(self, path_or_id: str) -> Optional[Knowledge]:
        """
        è·å–çŸ¥è¯†å†…å®¹
        
        Args:
            path_or_id: æ–‡ä»¶è·¯å¾„æˆ–çŸ¥è¯†ID
        """
        # å¦‚æœæ˜¯IDï¼Œå…ˆæŸ¥ç¼“å­˜
        if path_or_id.startswith("kb_"):
            if path_or_id in self._cache:
                return self._cache[path_or_id]
            # æœç´¢æ–‡ä»¶
            for md_file in self.base_path.rglob("*.md"):
                if md_file.name.startswith("_"):
                    continue
                knowledge = self._read_knowledge_file(md_file)
                if knowledge and knowledge.id == path_or_id:
                    self._cache[path_or_id] = knowledge
                    return knowledge
            return None
        
        # å¦‚æœæ˜¯è·¯å¾„
        file_path = Path(path_or_id)
        if not file_path.exists():
            file_path = self.base_path / path_or_id
        
        if file_path.exists():
            return self._read_knowledge_file(file_path)
        
        return None
    
    def get_stats(self, domain: str = None) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total': 0,
            'by_domain': {},
            'by_category': {},
            'recent': []
        }
        
        search_path = self.base_path / domain if domain else self.base_path
        
        if not search_path.exists():
            return stats
        
        files_with_time = []
        
        for md_file in search_path.rglob("*.md"):
            if md_file.name.startswith("_"):
                continue
            
            stats['total'] += 1
            
            # æŒ‰é¢†åŸŸç»Ÿè®¡
            file_domain = self._get_domain_from_path(md_file)
            stats['by_domain'][file_domain] = stats['by_domain'].get(file_domain, 0) + 1
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            file_category = self._get_category_from_path(md_file)
            key = f"{file_domain}/{file_category}"
            stats['by_category'][key] = stats['by_category'].get(key, 0) + 1
            
            # è®°å½•æ–‡ä»¶æ—¶é—´
            mtime = md_file.stat().st_mtime
            files_with_time.append((md_file, mtime))
        
        # æœ€è¿‘æ–‡ä»¶
        files_with_time.sort(key=lambda x: x[1], reverse=True)
        for md_file, _ in files_with_time[:5]:
            content = md_file.read_text(encoding='utf-8')
            stats['recent'].append({
                'path': str(md_file.relative_to(self.base_path)),
                'title': self._extract_title(content)
            })
        
        return stats
    
    def _generate_markdown(self, knowledge: Knowledge) -> str:
        """ç”Ÿæˆ Markdown æ–‡ä»¶å†…å®¹"""
        # YAML frontmatter
        frontmatter_lines = [
            "---",
            f"id: {knowledge.id}",
            f"title: \"{knowledge.title}\"",
            f"domain: {knowledge.domain}",
            f"category: {knowledge.category}",
            f"keywords: {knowledge.keywords}",
            f"source_url: \"{knowledge.source_url}\"",
            f"source_type: {knowledge.source_type}",
            f"acquired_by: {knowledge.acquired_by}",
            f"quality_score: {knowledge.quality_score}",
            f"created_at: {knowledge.created_at}",
            f"updated_at: {knowledge.updated_at}",
            f"access_count: {knowledge.access_count}",
            "---",
            ""
        ]
        
        # æ­£æ–‡å†…å®¹
        content_lines = [
            f"# {knowledge.title}",
            "",
            "## æ¥æºä¿¡æ¯",
            "",
            f"- **è·å–æ—¶é—´**: {knowledge.created_at[:10]}",
            f"- **è·å–æŠ€èƒ½**: {knowledge.acquired_by or 'æ‰‹åŠ¨æ·»åŠ '}",
            f"- **åŸå§‹é“¾æ¥**: {knowledge.source_url or 'æ— '}",
            "",
            "## å†…å®¹",
            "",
            knowledge.content,
            "",
            "## å…³é”®è¯",
            "",
            ", ".join(knowledge.keywords) if knowledge.keywords else "æ— ",
            ""
        ]
        
        return "\n".join(frontmatter_lines + content_lines)
    
    def _read_knowledge_file(self, file_path: Path) -> Optional[Knowledge]:
        """ä»æ–‡ä»¶è¯»å–çŸ¥è¯†"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # è§£æ frontmatter
            if content.startswith("---"):
                parts = content.split("---", 2)
                if len(parts) >= 3:
                    frontmatter = parts[1].strip()
                    body = parts[2].strip()
                    
                    # ç®€å•è§£æ YAML
                    meta = {}
                    for line in frontmatter.split("\n"):
                        if ":" in line:
                            key, value = line.split(":", 1)
                            key = key.strip()
                            value = value.strip().strip('"\'')
                            
                            # å¤„ç†åˆ—è¡¨
                            if value.startswith("[") and value.endswith("]"):
                                value = [v.strip().strip('"\'') 
                                        for v in value[1:-1].split(",") if v.strip()]
                            # å¤„ç†æ•°å­—
                            elif value.replace(".", "").isdigit():
                                value = float(value) if "." in value else int(value)
                            
                            meta[key] = value
                    
                    return Knowledge(
                        id=meta.get('id', ''),
                        title=meta.get('title', ''),
                        content=body,
                        domain=meta.get('domain', ''),
                        category=meta.get('category', ''),
                        keywords=meta.get('keywords', []),
                        source_url=meta.get('source_url', ''),
                        source_type=meta.get('source_type', ''),
                        acquired_by=meta.get('acquired_by', ''),
                        quality_score=meta.get('quality_score', 0.5),
                        created_at=meta.get('created_at', ''),
                        updated_at=meta.get('updated_at', ''),
                        access_count=meta.get('access_count', 0)
                    )
        except Exception as e:
            self.logger.warning(f"è§£æçŸ¥è¯†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return None
    
    def _update_index(self):
        """æ›´æ–°ç´¢å¼•æ–‡ä»¶"""
        stats = self.get_stats()
        
        index_lines = [
            "# çŸ¥è¯†åº“ç´¢å¼•",
            "",
            f"> æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"> æ€»çŸ¥è¯†æ•°: {stats['total']}",
            "",
            "## æŒ‰é¢†åŸŸç»Ÿè®¡",
            "",
            "| é¢†åŸŸ | æ•°é‡ |",
            "|------|------|",
        ]
        
        for domain, count in stats['by_domain'].items():
            index_lines.append(f"| {domain} | {count} |")
        
        index_lines.extend([
            "",
            "## æœ€è¿‘è·å–",
            ""
        ])
        
        for i, item in enumerate(stats['recent'], 1):
            index_lines.append(f"{i}. [{item['title']}]({item['path']})")
        
        index_content = "\n".join(index_lines)
        index_path = self.base_path / "_index.md"
        index_path.write_text(index_content, encoding='utf-8')
    
    def _safe_filename(self, title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤ä¸å®‰å…¨å­—ç¬¦
        safe = re.sub(r'[<>:"/\\|?*]', '', title)
        # æ›¿æ¢ç©ºæ ¼
        safe = safe.replace(' ', '_')
        # é™åˆ¶é•¿åº¦
        if len(safe) > 50:
            safe = safe[:50]
        # ç¡®ä¿ä¸ä¸ºç©º
        if not safe:
            safe = hashlib.md5(title.encode()).hexdigest()[:8]
        return safe
    
    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """ä»æ–‡æœ¬æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆä¸­æ–‡åˆ†è¯ç®€åŒ–ç‰ˆï¼‰
        # æå–2-4å­—çš„ä¸­æ–‡è¯
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
        
        # ç»Ÿè®¡è¯é¢‘
        word_count = {}
        for word in chinese_words:
            word_count[word] = word_count.get(word, 0) + 1
        
        # æŒ‰è¯é¢‘æ’åº
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'äº†', 'æœ‰', 'è¢«', 'å¯¹', 'ç­‰', 'ä¸º', 'ä»¥'}
        keywords = [w for w, _ in sorted_words if w not in stopwords]
        
        return keywords[:max_keywords]
    
    def _extract_title(self, content: str) -> str:
        """ä»å†…å®¹æå–æ ‡é¢˜"""
        # å…ˆå°è¯•ä» frontmatter æå–
        title = self._extract_frontmatter_field(content, 'title')
        if title:
            return title
        
        # å°è¯•ä» # æ ‡é¢˜æå–
        match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
        
        # è¿”å›å‰30ä¸ªå­—ç¬¦
        return content[:30].replace('\n', ' ').strip() + "..."
    
    def _extract_frontmatter_field(self, content: str, field: str) -> Any:
        """ä» frontmatter æå–å­—æ®µ"""
        if not content.startswith("---"):
            return None
        
        parts = content.split("---", 2)
        if len(parts) < 3:
            return None
        
        frontmatter = parts[1]
        for line in frontmatter.split("\n"):
            if line.startswith(f"{field}:"):
                value = line.split(":", 1)[1].strip().strip('"\'')
                # å¤„ç†åˆ—è¡¨
                if value.startswith("["):
                    return [v.strip().strip('"\'') 
                           for v in value[1:-1].split(",") if v.strip()]
                return value
        
        return None
    
    def _extract_snippet(self, content: str, query: str, context_chars: int = 100) -> str:
        """æå–åŒ…å«æŸ¥è¯¢è¯çš„ç‰‡æ®µ"""
        # è·³è¿‡ frontmatter
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 3:
                content = parts[2]
        
        query_lower = query.lower()
        content_lower = content.lower()
        
        pos = content_lower.find(query_lower)
        if pos == -1:
            # å°è¯•æŸ¥æ‰¾æŸ¥è¯¢è¯çš„ä¸€éƒ¨åˆ†
            for word in query_lower.split():
                pos = content_lower.find(word)
                if pos != -1:
                    break
        
        if pos == -1:
            return content[:context_chars * 2] + "..."
        
        start = max(0, pos - context_chars)
        end = min(len(content), pos + len(query) + context_chars)
        
        snippet = content[start:end]
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."
        
        return snippet.replace('\n', ' ').strip()
    
    def _get_domain_from_path(self, file_path: Path) -> str:
        """ä»è·¯å¾„è·å–é¢†åŸŸ"""
        try:
            rel_path = file_path.relative_to(self.base_path)
            parts = rel_path.parts
            if len(parts) >= 1:
                return parts[0]
        except ValueError:
            pass
        return "unknown"
    
    def _get_category_from_path(self, file_path: Path) -> str:
        """ä»è·¯å¾„è·å–ç±»åˆ«"""
        try:
            rel_path = file_path.relative_to(self.base_path)
            parts = rel_path.parts
            if len(parts) >= 2:
                return parts[1]
        except ValueError:
            pass
        return "general"
    
    def _is_similar(self, existing: Knowledge, new: Knowledge) -> bool:
        """æ£€æŸ¥ä¸¤æ¡çŸ¥è¯†æ˜¯å¦ç›¸ä¼¼ï¼ˆå»é‡ç”¨ï¼‰"""
        # æ ‡é¢˜å®Œå…¨ç›¸åŒ
        if existing.title == new.title:
            return True
        # URL ç›¸åŒ
        if existing.source_url and existing.source_url == new.source_url:
            return True
        return False
    
    def _update_access(self, file_path: Path):
        """æ›´æ–°è®¿é—®ç»Ÿè®¡"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # æ›´æ–° access_count
            content = re.sub(
                r'access_count: (\d+)',
                lambda m: f'access_count: {int(m.group(1)) + 1}',
                content
            )
            
            # æ›´æ–° last_accessed
            now = datetime.now().isoformat()
            if 'last_accessed:' in content:
                content = re.sub(
                    r'last_accessed: .*',
                    f'last_accessed: {now}',
                    content
                )
            
            file_path.write_text(content, encoding='utf-8')
        except Exception as e:
            self.logger.warning(f"æ›´æ–°è®¿é—®ç»Ÿè®¡å¤±è´¥: {e}")


# ä¾¿æ·å‡½æ•°
_default_kb: Optional[MarkdownKnowledge] = None

def get_knowledge_base(base_path: str = "prokaryote_agent/knowledge") -> MarkdownKnowledge:
    """è·å–é»˜è®¤çŸ¥è¯†åº“å®ä¾‹"""
    global _default_kb
    if _default_kb is None:
        _default_kb = MarkdownKnowledge(base_path)
    return _default_kb

def store_knowledge(title: str, content: str, domain: str, category: str, 
                    source_url: str = "", acquired_by: str = "") -> str:
    """å­˜å‚¨çŸ¥è¯†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    kb = get_knowledge_base()
    return kb.store_from_search(title, content, domain, category, source_url, acquired_by)

def search_knowledge(query: str, domain: str = None, limit: int = 10) -> List[Dict]:
    """æœç´¢çŸ¥è¯†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    kb = get_knowledge_base()
    return kb.search(query, domain=domain, limit=limit)


def smart_search(query: str, domain: str, min_local: int = 2,
                 web_search_func=None, acquired_by: str = "") -> Dict[str, Any]:
    """
    æ™ºèƒ½æœç´¢ - ä¼˜å…ˆæœ¬åœ°çŸ¥è¯†åº“ï¼Œä¸è¶³æ—¶è¡¥å……ç½‘ç»œæœç´¢å¹¶å›ºåŒ–çŸ¥è¯†
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        domain: é¢†åŸŸ ('legal', 'software_dev' ç­‰)
        min_local: æœ¬åœ°ç»“æœæœ€å°æ•°é‡ï¼Œä¸è¶³æ—¶è§¦å‘ç½‘ç»œæœç´¢
        web_search_func: ç½‘ç»œæœç´¢å‡½æ•°ï¼Œç­¾å: func(query, max_results) -> List[Dict]
        acquired_by: è·å–çŸ¥è¯†çš„æŠ€èƒ½ID
        
    Returns:
        {
            'local_results': [...],   # æœ¬åœ°çŸ¥è¯†
            'web_results': [...],     # ç½‘ç»œæœç´¢ç»“æœ
            'all_results': [...],     # åˆå¹¶åçš„ç»“æœ
            'total': int,             # æ€»æ•°
            'from_local': int,        # æ¥è‡ªæœ¬åœ°çš„æ•°é‡
            'from_web': int,          # æ¥è‡ªç½‘ç»œçš„æ•°é‡
            'knowledge_stored': int   # æ–°å­˜å‚¨çš„çŸ¥è¯†æ•°é‡
        }
    """
    logger = logging.getLogger(__name__)
    
    result = {
        'local_results': [],
        'web_results': [],
        'all_results': [],
        'total': 0,
        'from_local': 0,
        'from_web': 0,
        'knowledge_stored': 0
    }
    
    # 1. å…ˆæœç´¢æœ¬åœ°çŸ¥è¯†åº“
    local_results = search_knowledge(query, domain=domain, limit=5)
    result['local_results'] = local_results
    result['from_local'] = len(local_results)
    
    if local_results:
        logger.info(f"ğŸ“š çŸ¥è¯†åº“å‘½ä¸­ '{query}': {len(local_results)} æ¡")
    
    # 2. å¦‚æœæœ¬åœ°çŸ¥è¯†ä¸è¶³ï¼Œè¿›è¡Œç½‘ç»œæœç´¢
    if len(local_results) < min_local and web_search_func:
        try:
            web_results = web_search_func(query, max_results=5)
            if web_results:
                result['web_results'] = web_results
                result['from_web'] = len(web_results)
                
                logger.info(f"ğŸŒ ç½‘ç»œæœç´¢ '{query}': {len(web_results)} æ¡")
                
                # 3. å°†æœ‰ä»·å€¼çš„ç½‘ç»œç»“æœå­˜å…¥çŸ¥è¯†åº“
                stored_count = 0
                for item in web_results:
                    if _is_valuable_knowledge(item):
                        category = _determine_category(item, domain)
                        try:
                            path = store_knowledge(
                                title=item.get('title', ''),
                                content=item.get('content', item.get('snippet', '')),
                                domain=domain,
                                category=category,
                                source_url=item.get('url', ''),
                                acquired_by=acquired_by
                            )
                            if path:
                                stored_count += 1
                        except Exception as e:
                            logger.debug(f"å­˜å‚¨çŸ¥è¯†å¤±è´¥: {e}")
                
                if stored_count > 0:
                    logger.info(f"ğŸ’¾ çŸ¥è¯†å›ºåŒ–: {stored_count} æ¡æ–°çŸ¥è¯†å·²å­˜å‚¨")
                result['knowledge_stored'] = stored_count
                
        except Exception as e:
            logger.warning(f"ç½‘ç»œæœç´¢å¤±è´¥: {e}")
    
    # åˆå¹¶ç»“æœ
    result['all_results'] = local_results + result['web_results']
    result['total'] = result['from_local'] + result['from_web']
    return result


def _is_valuable_knowledge(item: Dict[str, Any]) -> bool:
    """è¯„ä¼°çŸ¥è¯†æ˜¯å¦æœ‰å­˜å‚¨ä»·å€¼"""
    content = item.get('content', item.get('snippet', ''))
    
    # å†…å®¹é•¿åº¦æ£€æŸ¥
    if not content or len(content) < 80:
        return False
    
    # æ¥æºæ£€æŸ¥ï¼ˆå®˜æ–¹ç½‘ç«™ä¼˜å…ˆï¼‰
    url = item.get('url', '')
    valuable_domains = [
        'gov.cn', 'court.gov.cn', 'law.cn',  # æ³•å¾‹å®˜æ–¹
        'wikipedia.org', 'baike.baidu.com',   # ç™¾ç§‘
        'github.com', 'docs.python.org',      # æŠ€æœ¯æ–‡æ¡£
    ]
    
    # å¦‚æœæ˜¯å¯ä¿¡æ¥æºï¼Œé™ä½å†…å®¹é•¿åº¦è¦æ±‚
    for domain in valuable_domains:
        if domain in url:
            return len(content) >= 50
    
    return True


def _determine_category(item: Dict[str, Any], domain: str) -> str:
    """æ ¹æ®å†…å®¹ç¡®å®šçŸ¥è¯†ç±»åˆ«"""
    content = (item.get('title', '') + ' ' + item.get('content', '')).lower()
    url = item.get('url', '').lower()
    
    if domain == 'legal':
        if 'åˆ¤ä¾‹' in content or 'case' in url or 'æ¡ˆä¾‹' in content:
            return 'cases'
        elif 'æ³•' in content or 'æ¡ä¾‹' in content or 'è§„å®š' in content:
            return 'laws'
        else:
            return 'concepts'
    
    elif domain == 'software_dev':
        if 'error' in content or 'é”™è¯¯' in content or 'exception' in content:
            return 'errors'
        elif 'api' in url or 'doc' in url:
            return 'apis'
        else:
            return 'general'
    
    return 'general'


def get_knowledge_stats(domain: str = None) -> Dict[str, Any]:
    """
    è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        domain: é™å®šé¢†åŸŸï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    kb = get_knowledge_base()
    
    stats = {
        'total_files': 0,
        'by_domain': {},
        'by_category': {}
    }
    
    # ç»Ÿè®¡æ–‡ä»¶æ•°
    for md_file in kb.base_path.rglob("*.md"):
        if md_file.name.startswith("_"):
            continue
        
        try:
            rel_path = md_file.relative_to(kb.base_path)
            parts = rel_path.parts
            if len(parts) >= 2:
                file_domain = parts[0]
                file_category = parts[1]
                
                if domain and file_domain != domain:
                    continue
                
                stats['total_files'] += 1
                stats['by_domain'][file_domain] = stats['by_domain'].get(file_domain, 0) + 1
                stats['by_category'][file_category] = stats['by_category'].get(file_category, 0) + 1
        except ValueError:
            pass
    
    return stats
